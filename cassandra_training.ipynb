{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:30.325266Z",
     "start_time": "2025-04-12T16:19:30.322609Z"
    },
    "id": "tuVeBiVtg3za"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 17:53:58.931327: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcudart.so.12\n",
      "2025-04-13 17:53:58.931399: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:861] GCS cache max size = 0 ; block size = 67108864 ; max staleness = 0\n",
      "2025-04-13 17:53:58.931406: I external/local_xla/xla/tsl/platform/cloud/ram_file_block_cache.h:64] GCS file block cache is disabled\n",
      "2025-04-13 17:53:58.931409: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:901] GCS DNS cache is disabled, because GCS_RESOLVE_REFRESH_SECS = 0 (or is not set)\n",
      "2025-04-13 17:53:58.931411: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:931] GCS additional header DISABLED. No environment variable set.\n",
      "2025-04-13 17:53:58.931413: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:310] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10\n",
      "2025-04-13 17:53:58.931415: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:310] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10\n",
      "2025-04-13 17:53:58.934252: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcudart.so.12\n",
      "2025-04-13 17:53:58.941274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744566838.955611      85 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744566838.959187      85 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744566838.970941      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744566838.970961      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744566838.970963      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744566838.970964      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-13 17:53:58.973472: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:861] GCS cache max size = 0 ; block size = 67108864 ; max staleness = 0\n",
      "2025-04-13 17:53:58.973484: I external/local_xla/xla/tsl/platform/cloud/ram_file_block_cache.h:64] GCS file block cache is disabled\n",
      "2025-04-13 17:53:58.973487: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:901] GCS DNS cache is disabled, because GCS_RESOLVE_REFRESH_SECS = 0 (or is not set)\n",
      "2025-04-13 17:53:58.973488: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:931] GCS additional header DISABLED. No environment variable set.\n",
      "2025-04-13 17:53:58.973491: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:310] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10\n",
      "2025-04-13 17:53:58.973493: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:310] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10\n",
      "2025-04-13 17:53:58.974685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-13 17:54:00.155297: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:861] GCS cache max size = 0 ; block size = 67108864 ; max staleness = 0\n",
      "2025-04-13 17:54:00.155320: I external/local_xla/xla/tsl/platform/cloud/ram_file_block_cache.h:64] GCS file block cache is disabled\n",
      "2025-04-13 17:54:00.155326: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:901] GCS DNS cache is disabled, because GCS_RESOLVE_REFRESH_SECS = 0 (or is not set)\n",
      "2025-04-13 17:54:00.155329: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:931] GCS additional header DISABLED. No environment variable set.\n",
      "2025-04-13 17:54:00.155333: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:310] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10\n",
      "2025-04-13 17:54:00.155336: I external/local_xla/xla/tsl/platform/cloud/gcs_file_system.cc:310] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7410987409845107341\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5816254464\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4606508342838572039\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:07:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 17:54:00.790134: I tensorflow/core/common_runtime/local_device.cc:162] LocalDevice using CPU work thread pool: 0x5b145d718fa0, num_threads=12\n",
      "2025-04-13 17:54:00.790200: I external/local_xla/xla/parse_flags_from_env.cc:213] For env var TF_XLA_FLAGS found arguments:\n",
      "2025-04-13 17:54:00.790203: I external/local_xla/xla/parse_flags_from_env.cc:215]   argv[0] = <argv[0]>\n",
      "2025-04-13 17:54:00.790207: I external/local_xla/xla/parse_flags_from_env.cc:213] For env var TF_JITRT_FLAGS found arguments:\n",
      "2025-04-13 17:54:00.790209: I external/local_xla/xla/parse_flags_from_env.cc:215]   argv[0] = <argv[0]>\n",
      "2025-04-13 17:54:00.790212: I tensorflow/compiler/jit/xla_cpu_device.cc:63] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-04-13 17:54:00.790214: I tensorflow/compiler/jit/xla_gpu_device.cc:83] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-04-13 17:54:00.790850: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcuda.so.1\n",
      "2025-04-13 17:54:00.810172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] CreateGPUDevice: num_gpus_to_use: 2147483647, visible_device_list: , populate_pjrt_gpu_client_creation_info: 0\n",
      "2025-04-13 17:54:00.810403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2310] Found device 0 with properties: \n",
      "pciBusID: 0000:07:00.0 name: NVIDIA GeForce RTX 3070 computeCapability: 8.6\n",
      "coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 7.78GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2025-04-13 17:54:00.810412: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcudart.so.12\n",
      "2025-04-13 17:54:00.837424: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcublas.so.12\n",
      "2025-04-13 17:54:00.837464: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcublasLt.so.12\n",
      "2025-04-13 17:54:00.839268: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcufft.so.11\n",
      "2025-04-13 17:54:00.842200: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcusolver.so.11\n",
      "2025-04-13 17:54:00.842232: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcusparse.so.12\n",
      "2025-04-13 17:54:00.842346: I external/local_xla/xla/tsl/platform/default/dso_loader.cc:73] Successfully opened dynamic library libcudnn.so.9\n",
      "2025-04-13 17:54:00.842614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2444] Adding visible gpu devices: 0\n",
      "2025-04-13 17:54:00.898162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1503] Cuda stream priority range on GPU(0): -5,0\n",
      "2025-04-13 17:54:00.898181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1560] TensorFlow compiled with CUDA 12.5 and cuDNN 9.3.0\n",
      "2025-04-13 17:54:00.898186: I external/local_xla/xla/stream_executor/executor_cache.cc:43] building executor\n",
      "2025-04-13 17:54:00.898199: I external/local_xla/xla/stream_executor/cuda/cuda_context.cc:160] The primary context 0x5b145dc5d9c0 for device 0 exists before initializing the StreamExecutor.\n",
      "2025-04-13 17:54:00.898203: I external/local_xla/xla/stream_executor/cuda/cuda_context.cc:181] created or reused context 0x5b145dc5d9c0 for this thread\n",
      "2025-04-13 17:54:00.898208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1572] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-04-13 17:54:00.898211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1578]      0 \n",
      "2025-04-13 17:54:00.898212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1591] 0:   N \n",
      "2025-04-13 17:54:00.898216: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.898701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2153] GPUDevice PlatformDeviceId 0 TfDeviceId 0 on bus 1 numa: 0 pci: 0000:07:00.0 DeviceLocality: bus_id: 1\n",
      "links {\n",
      "}\n",
      "\n",
      "2025-04-13 17:54:00.898712: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-04-13 17:54:00.898716: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:88] Creating new BFCAllocator named: GPU_0_bfc\n",
      "2025-04-13 17:54:00.898720: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 256B\n",
      "2025-04-13 17:54:00.898722: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 512B\n",
      "2025-04-13 17:54:00.898727: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 1.0KiB\n",
      "2025-04-13 17:54:00.898729: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 2.0KiB\n",
      "2025-04-13 17:54:00.898731: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 4.0KiB\n",
      "2025-04-13 17:54:00.898733: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 8.0KiB\n",
      "2025-04-13 17:54:00.898736: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 16.0KiB\n",
      "2025-04-13 17:54:00.898737: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 32.0KiB\n",
      "2025-04-13 17:54:00.898739: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 64.0KiB\n",
      "2025-04-13 17:54:00.898741: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 128.0KiB\n",
      "2025-04-13 17:54:00.898743: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 256.0KiB\n",
      "2025-04-13 17:54:00.898745: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 512.0KiB\n",
      "2025-04-13 17:54:00.898747: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 1.00MiB\n",
      "2025-04-13 17:54:00.898749: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 2.00MiB\n",
      "2025-04-13 17:54:00.898750: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 4.00MiB\n",
      "2025-04-13 17:54:00.898752: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 8.00MiB\n",
      "2025-04-13 17:54:00.898754: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 16.00MiB\n",
      "2025-04-13 17:54:00.898756: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 32.00MiB\n",
      "2025-04-13 17:54:00.898758: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 64.00MiB\n",
      "2025-04-13 17:54:00.898759: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 128.00MiB\n",
      "2025-04-13 17:54:00.898761: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 256.00MiB\n",
      "I0000 00:00:1744566840.898764      85 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0\n",
      "2025-04-13 17:54:00.898772: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:110] Number of regions allocated: 0\n",
      "2025-04-13 17:54:00.898776: I external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:130] DRIVER VERSION: 12040\n",
      "2025-04-13 17:54:00.898777: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.898837: I external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:196] using default memory pool 0x5b145b322c18\n",
      "2025-04-13 17:54:00.898846: I external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:199] gpu_async_0 CudaMallocAsync initialized on platform: 0 with pool size of: 5816254464 this ptr: 0x5b145b323a80\n",
      "2025-04-13 17:54:00.898857: I external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:308] gpu_async_0 GpuCudaMallocAsyncAllocator PoolSize 5816254464\n",
      "2025-04-13 17:54:00.898983: I tensorflow/core/common_runtime/local_device.cc:162] LocalDevice using CPU work thread pool: 0x5b145d718fa0, num_threads=12\n",
      "I0000 00:00:1744566840.898995      85 gpu_device.cc:2019] Created device /device:GPU:0 with 5546 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2025-04-13 17:54:00.899002: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899017: I external/local_xla/xla/stream_executor/cuda/cuda_stream.cc:98] successfully created stream 0x5b1458e1f060 for executor 0x5b145d49ca40 on thread\n",
      "2025-04-13 17:54:00.899019: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:311] Created stream[0] = 0x5b1458e199e0 with priority: 0\n",
      "2025-04-13 17:54:00.899028: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899034: I external/local_xla/xla/stream_executor/cuda/cuda_stream.cc:98] successfully created stream 0x5b1458a80520 for executor 0x5b145d49ca40 on thread\n",
      "2025-04-13 17:54:00.899036: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:328] Created host_to_device_stream[0] = 0x5b1458e146b0\n",
      "2025-04-13 17:54:00.899040: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899044: I external/local_xla/xla/stream_executor/cuda/cuda_stream.cc:98] successfully created stream 0x5b145b321000 for executor 0x5b145d49ca40 on thread\n",
      "2025-04-13 17:54:00.899045: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:332] Created device_to_host_stream[0] = 0x5b1458df7320\n",
      "2025-04-13 17:54:00.899049: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899052: I external/local_xla/xla/stream_executor/cuda/cuda_stream.cc:98] successfully created stream 0x5b145b320590 for executor 0x5b145d49ca40 on thread\n",
      "2025-04-13 17:54:00.899054: I external/local_xla/xla/stream_executor/gpu/scoped_activate_context.cc:41] ScopedActivateContext switching to 0\n",
      "2025-04-13 17:54:00.899056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:347] Created device_to_device_stream[0] = 0x5b1458e24260\n",
      "2025-04-13 17:54:00.899059: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:88] Creating new BFCAllocator named: gpu_host_bfc\n",
      "2025-04-13 17:54:00.899062: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 256B\n",
      "2025-04-13 17:54:00.899064: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 512B\n",
      "2025-04-13 17:54:00.899066: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 1.0KiB\n",
      "2025-04-13 17:54:00.899068: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 2.0KiB\n",
      "2025-04-13 17:54:00.899070: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 4.0KiB\n",
      "2025-04-13 17:54:00.899072: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 8.0KiB\n",
      "2025-04-13 17:54:00.899074: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 16.0KiB\n",
      "2025-04-13 17:54:00.899076: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 32.0KiB\n",
      "2025-04-13 17:54:00.899077: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 64.0KiB\n",
      "2025-04-13 17:54:00.899079: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 128.0KiB\n",
      "2025-04-13 17:54:00.899081: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 256.0KiB\n",
      "2025-04-13 17:54:00.899083: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 512.0KiB\n",
      "2025-04-13 17:54:00.899086: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 1.00MiB\n",
      "2025-04-13 17:54:00.899088: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 2.00MiB\n",
      "2025-04-13 17:54:00.899089: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 4.00MiB\n",
      "2025-04-13 17:54:00.899091: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 8.00MiB\n",
      "2025-04-13 17:54:00.899093: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 16.00MiB\n",
      "2025-04-13 17:54:00.899095: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 32.00MiB\n",
      "2025-04-13 17:54:00.899097: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 64.00MiB\n",
      "2025-04-13 17:54:00.899098: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 128.00MiB\n",
      "2025-04-13 17:54:00.899100: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:91] Creating bin of max chunk size 256.00MiB\n",
      "2025-04-13 17:54:00.900127: I tensorflow/compiler/jit/xla_cpu_device.cc:49] Not creating XLA devices, tf_xla_enable_xla_devices not set and XLA device creation not requested\n",
      "2025-04-13 17:54:00.900133: I tensorflow/compiler/jit/xla_gpu_device.cc:53] Not creating XLA devices, tf_xla_enable_xla_devices not set and XLA devices creation not required\n",
      "2025-04-13 17:54:00.900259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2310] Found device 0 with properties: \n",
      "pciBusID: 0000:07:00.0 name: NVIDIA GeForce RTX 3070 computeCapability: 8.6\n",
      "coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 7.78GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2025-04-13 17:54:00.900471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2444] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KyQ4xDxxInC"
   },
   "source": [
    "### **Basics - Classes and Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:30.571522Z",
     "start_time": "2025-04-12T16:19:30.533588Z"
    },
    "id": "7VDLwT4QqYBE",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def norm1( data ):\n",
    "      norms = np.abs( data.sum(axis=1) )\n",
    "      norms[norms==0] = 1\n",
    "      return data/norms[:,None]\n",
    "\n",
    "def totalEnergy( data ):\n",
    "    norms = data.sum(axis=1)\n",
    "    norms[norms==0] = 1\n",
    "\n",
    "    return data/norms[:,None]\n",
    "\n",
    "def meanStd( data ):\n",
    "    mean_data = np.mean(data,axis=1)\n",
    "    std_data = np.std(data,axis=1)\n",
    "    data_norm = (np.transpose(data) - mean_data)/std_data\n",
    "    data_norm = np.transpose(data_norm)\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "# Topological Preprocessing\n",
    "class RpLayer(Layer):\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super(RpLayer, self).__init__(**kwargs)\n",
    "    self.rvec = np.concatenate((np.arange(1,9),np.arange(1,65),np.arange(1,9),np.arange(1,9),np.arange(1,5),np.arange(1,5),np.arange(1,5)))\n",
    "    self.output_dim = (len(self.rvec),)\n",
    "\n",
    "\n",
    "  def build( self, input_shape ):\n",
    "\n",
    "    # Create the alpha trainable tf.variable\n",
    "    self.__alpha = self.add_weight( name='alpha',\n",
    "                               shape=(1,1),\n",
    "                               initializer=tf.keras.initializers.RandomNormal(mean=1, stddev=0.00005),\n",
    "                               trainable=True)\n",
    "\n",
    "    # Create the beta trainable tf.variable\n",
    "    self.__beta = self.add_weight(name='beta',\n",
    "                                  shape=(1,1),\n",
    "                                  initializer=tf.keras.initializers.RandomNormal(mean=1, stddev=0.00005),\n",
    "                                  trainable=True)\n",
    "\n",
    "    # Create the rvec tf.constant\n",
    "    self.__rvec = K.constant( copy(self.rvec) )\n",
    "    super(RpLayer, self).build(input_shape)\n",
    "\n",
    "\n",
    "  def call(self, input):\n",
    "\n",
    "    Ea = K.sign(input)*K.pow( K.abs(input), self.__alpha )\n",
    "    rb =  K.pow(self.__rvec, self.__beta)\n",
    "    Ea_sum = tf.reshape( K.sum( Ea, axis=1), (-1,1))\n",
    "    out = (Ea*rb)/Ea_sum\n",
    "    return out\n",
    "\n",
    "  def get_output_shape_for(self, input_shape):\n",
    "    return self.output_dim\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "class sp(Callback):\n",
    "\n",
    "  def __init__(self, verbose=True,save_the_best=True,rp_layer=True,patience=True, **kw):\n",
    "    super(Callback, self).__init__()\n",
    "    self.__verbose = verbose\n",
    "    self.__patience = 10\n",
    "    self.__ipatience = 0\n",
    "    self.__best_sp = 0.0\n",
    "    self.__save_the_best = save_the_best\n",
    "    self.__best_weights = None\n",
    "    self.__best_epoch = 0\n",
    "    self._validation_data = None\n",
    "    self.__rp_layer = rp_layer\n",
    "\n",
    "    if(self.__rp_layer):\n",
    "      self.__alpha = 1\n",
    "      self.__beta = 1\n",
    "\n",
    "  def set_validation_data( self, v ):\n",
    "    self._validation_data = v\n",
    "\n",
    "  # This computes dSP/dFA (partial derivative of SP respect to FA)\n",
    "  def __get_partial_derivative_fa (self, fa, pd):\n",
    "    c = 0.353553\n",
    "    up = -(pd * (pd - fa + 1))/(2 * np.sqrt(pd*(1-fa))) - np.sqrt(pd*(1-fa))\n",
    "    down = np.sqrt( np.sqrt(pd * (1-fa)) * (pd - fa + 1) )\n",
    "    return c * up / down\n",
    "\n",
    "  # This computes dSP/dPD (partial derivative of SP respect to PD)\n",
    "  def __get_partial_derivative_pd (self, fa, pd):\n",
    "    c = 0.353553\n",
    "    up = ((1-fa)*(pd-fa+1))/(2*np.sqrt(pd*(1-fa))) + np.sqrt(pd*(1-fa))\n",
    "    down = np.sqrt( np.sqrt(pd*(1-fa)) * (pd-fa+1) )\n",
    "    return c * up / down\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "    if self._validation_data is not None: # Check if _validation_data is set\n",
    "      y_true = self._validation_data[1]\n",
    "      y_pred = self.model.predict(self._validation_data[0],batch_size=1024).ravel()\n",
    "    else:\n",
    "      raise ValueError(\"Validation data not set. Please call set_validation_data() before training.\") # Raise error if not set\n",
    "\n",
    "    # Computes SP\n",
    "    fa, pd, thresholds = roc_curve(y_true, y_pred)\n",
    "    sp = np.sqrt(  np.sqrt(pd*(1-fa)) * (0.5*(pd+(1-fa)))  )\n",
    "\n",
    "    knee = np.argmax(sp)\n",
    "\n",
    "    # Computes partial derivatives\n",
    "    partial_pd = self.__get_partial_derivative_pd(fa[knee], pd[knee])\n",
    "    partial_fa = self.__get_partial_derivative_fa(fa[knee], pd[knee])\n",
    "\n",
    "    logs['max_sp_val'] = sp[knee]\n",
    "    logs['max_sp_fa_val'] = fa[knee]\n",
    "    logs['max_sp_pd_val'] = pd[knee]\n",
    "    logs['max_sp_partial_derivative_fa_val'] = partial_fa\n",
    "    logs['max_sp_partial_derivative_pd_val'] = partial_pd\n",
    "\n",
    "    if(self.__rp_layer):\n",
    "      self.alpha_beta_history()\n",
    "      logs['alpha_training'] = self.__alpha\n",
    "      logs['beta_training'] = self.__beta\n",
    "\n",
    "    if self.__verbose:\n",
    "      if(self.__rp_layer):\n",
    "        print (\" - val_sp: {:.4f} (fa:{:.4f},pd:{:.4f}), patience: {}, dSP/dFA: {:.4f}, dSP/dPD: {:.4f}, alpha: {:.4f}, beta: {:.4f} \".format(sp[knee],\n",
    "        fa[knee],pd[knee], self.__ipatience, partial_fa, partial_pd,self.__alpha,self.__beta))\n",
    "\n",
    "      else:\n",
    "        print (\" - val_sp: {:.4f} (fa:{:.4f},pd:{:.4f}), patience: {}, dSP/dFA: {:.4f}, dSP/dPD: {:.4f} \".format(sp[knee],\n",
    "        fa[knee],pd[knee], self.__ipatience, partial_fa, partial_pd))\n",
    "\n",
    "\n",
    "    if sp[knee] > self.__best_sp:\n",
    "      self.__best_sp = sp[knee]\n",
    "      if self.__save_the_best:\n",
    "        self.__best_weights =  self.model.get_weights()\n",
    "        logs['max_sp_best_epoch_val'] = epoch\n",
    "      self.__ipatience = 0\n",
    "    else:\n",
    "      self.__ipatience += 1\n",
    "\n",
    "    if self.__ipatience > self.__patience:\n",
    "      self.model.stop_training = True\n",
    "      self.__ipatience = 0\n",
    "      self.__best_sp = 0.0\n",
    "      self.__best_epoch = 0\n",
    "\n",
    "\n",
    "  def alpha_beta_history(self, logs={}):\n",
    "    self.__alpha = self.model.trainable_weights[0][0][0].numpy()\n",
    "    self.__beta = self.model.trainable_weights[1][0][0].numpy()\n",
    "\n",
    "\n",
    "# Tensor flow based ML models\n",
    "\n",
    "def get_model_mlp_rp(neuron_min,neuron_max,numInit):\n",
    "    modelCol = []\n",
    "    for n in range(neuron_min,neuron_max+1):\n",
    "      for init in range(numInit):\n",
    "        inputs = layers.Input(shape=(100,), name='Input_rings')\n",
    "        input_rp = RpLayer()(inputs)\n",
    "        dense_rp = layers.Dense(n, activation='sigmoid', name='dense_rp_layer')(input_rp)\n",
    "        dense = layers.Dense(1,activation='linear', name='output_for_inference')(dense_rp)\n",
    "        outputs = layers.Activation('sigmoid', name='output_for_training')(dense)\n",
    "        model = tf.keras.Model(inputs, outputs, name = \"model\")\n",
    "        modelCol.append(model)\n",
    "\n",
    "    return modelCol\n",
    "\n",
    "def get_model_mlp(neuron_min, neuron_max, numInit):\n",
    "    modelCol = []\n",
    "    for n in range(neuron_min,neuron_max+1):\n",
    "      for init in range(numInit):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(n, input_shape=(100,), activation='tanh'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        modelCol.append(model)\n",
    "\n",
    "    return modelCol\n",
    "\n",
    "\n",
    "def get_model_conv(inputShape, kernel_size=2, use_l2=False,numInit=10 ):\n",
    "    # expect an input with 100 domensions (features)\n",
    "    modelCol = []\n",
    "    for init in range(numInit):\n",
    "      input = layers.Input(shape=(inputShape,), name = 'Input') # 0\n",
    "      input_reshape = layers.Reshape((inputShape,1), name='Reshape_layer')(input)\n",
    "      conv = layers.Conv1D(4, kernel_size = kernel_size, activation='relu', name = 'conv1d_layer_1')(input_reshape) # 1\n",
    "      conv = layers.Conv1D(8, kernel_size = kernel_size, activation='relu', name = 'conv1d_layer_2')(conv) # 2\n",
    "      conv = layers.Flatten(name='flatten')(conv) # 3\n",
    "      dense = layers.Dense(16, activation='relu', name='dense_layer')(conv) # 4\n",
    "      if use_l2:\n",
    "          dense = layers.Dense(1,activation='linear', name='output_for_inference', kernel_regularizer='l2', bias_regularizer='l2')(dense) # 5\n",
    "      else:\n",
    "          dense = layers.Dense(1,activation='linear', name='output_for_inference')(dense) # 5\n",
    "      output = layers.Activation('sigmoid', name='output_for_training')(dense) # 6\n",
    "      model = tf.keras.Model(input, output, name = \"model\")\n",
    "      modelCol.append(model)\n",
    "\n",
    "    return modelCol\n",
    "\n",
    "def get_model_conv_rp(inputShape, kernel_size=2, use_l2=False,numInit=10 ):\n",
    "    # expect an input with 100 domensions (features)\n",
    "    modelCol = []\n",
    "\n",
    "    for init in range(numInit):\n",
    "      inputs = layers.Input(shape=(inputShape,), name = 'Input') # 0\n",
    "      input_rp = RpLayer()(inputs)\n",
    "      input_reshape = layers.Reshape((inputShape,1), name='Reshape_layer')(input_rp)\n",
    "      conv = layers.Conv1D(4, kernel_size = kernel_size, activation='relu', name = 'conv1d_layer_1')(input_reshape) # 1\n",
    "      conv = layers.Conv1D(8, kernel_size = kernel_size, activation='relu', name = 'conv1d_layer_2')(conv) # 2\n",
    "      conv = layers.Flatten(name='flatten')(conv) # 3\n",
    "      dense = layers.Dense(16, activation='relu', name='dense_layer')(conv) # 4\n",
    "      if use_l2:\n",
    "          dense = layers.Dense(1,activation='linear', name='output_for_inference', kernel_regularizer='l2', bias_regularizer='l2')(dense) # 5\n",
    "      else:\n",
    "          dense = layers.Dense(1,activation='linear', name='output_for_inference')(dense) # 5\n",
    "      output = layers.Activation('sigmoid', name='output_for_training')(dense) # 6\n",
    "      model = tf.keras.Model(inputs, output, name = \"model_init%i_neuron%i\"%(init,16))\n",
    "      modelCol.append(model)\n",
    "\n",
    "    return modelCol\n",
    "\n",
    "\n",
    "# evaluation\n",
    "\n",
    "def sp_metrics(pf,pd):\n",
    "\n",
    "  sp = np.sqrt(  np.sqrt(pd*(1-fa)) * (0.5*(pd+(1-fa)))  )\n",
    "  sp_max = np.max(sp)\n",
    "\n",
    "  return sp_max, sp\n",
    "\n",
    "\n",
    "# Rp topological pre processing analysis\n",
    "\n",
    "def getBest_alphaBeta(history):\n",
    "\n",
    "    alpha = history['alpha_training'][-1]\n",
    "    beta = history['beta_training'][-1]\n",
    "\n",
    "\n",
    "    return alpha,beta\n",
    "\n",
    "def getBest_alphaBeta_history(history):\n",
    "    alpha_hist = history['alpha_training']\n",
    "    beta_hist = history['beta_training']\n",
    "\n",
    "\n",
    "    return alpha_hist,beta_hist\n",
    "\n",
    "\n",
    "def getPerfRef(model_perf,pd):\n",
    "  pf_ref = []\n",
    "  sp_ref = []\n",
    "  for i,k in enumerate(model_perf['roc_pd_test']):\n",
    "    if k >= pd:\n",
    "      pf_ref.append(model_perf['roc_pf_test'][i])\n",
    "      sp_ref.append(model_perf['sp_test'][i])\n",
    "\n",
    "      return model_perf['sp_test'][i]\n",
    "\n",
    "\n",
    "\n",
    "# dumping information\n",
    "\n",
    "def dumpModel(models,history,skf,x,y,seed_cv,numInit):\n",
    "    vars = ['model','fold','init','history','seed_cv','output_test','output_train',\n",
    "            'sp_test','sp_train','pd_test','pd_train','pf_test','pf_train',\n",
    "            'thr_train','thr_test','roc_sp_test','roc_sp_train','roc_pd_test',\n",
    "            'roc_pf_test','roc_pd_train','roc_pf_train','roc_thr_test','roc_thr_train',\n",
    "            'mse_train','mse_test','size_sig_test','size_bkg_test']\n",
    "\n",
    "    d = { key:[] for key in vars }\n",
    "    init = 0\n",
    "    for i, (train, test) in enumerate(skf.split(x, y)):\n",
    "\n",
    "\n",
    "        for model_idx, model in enumerate(models):\n",
    "\n",
    "          print(f\"Dumping Fold {i}:\")\n",
    "          print(f\"Dumping Init {init}:\")\n",
    "\n",
    "          y_pred_test = model.predict(x[test])\n",
    "          y_pred_train = model.predict(x[train])\n",
    "\n",
    "\n",
    "          d['model'].append(model.to_json()) # Store model configuration as JSON string\n",
    "          d['fold'].append(i)\n",
    "\n",
    "          d['history'].append(history[model_idx + i * len(models)].history) # Access history from the correct History object\n",
    "          d['seed_cv'].append(seed_cv)\n",
    "\n",
    "          fa, pd, thr = roc_curve(y[train], y_pred_train)\n",
    "          sp = np.sqrt(np.sqrt(pd*(1-fa)) * (0.5*(pd+(1-fa))))\n",
    "          knee = np.argmax(sp)\n",
    "\n",
    "          d['output_train'].append(y_pred_train.tolist()) # Convert to list\n",
    "          d['mse_train'].append(mean_squared_error(y[train], y_pred_train))\n",
    "          d['sp_train'].append(sp[knee])\n",
    "          d['pd_train'].append(pd[knee])\n",
    "          d['pf_train'].append(fa[knee])\n",
    "          d['thr_train'].append(thr[knee])\n",
    "          d['roc_sp_train'].append(sp.tolist()) # Convert to list\n",
    "          d['roc_pd_train'].append(pd.tolist()) # Convert to list\n",
    "          d['roc_pf_train'].append(fa.tolist()) # Convert to list\n",
    "          d['roc_thr_train'].append(thr.tolist()) # Convert to list\n",
    "\n",
    "          fa, pd, thr = roc_curve(y[test], y_pred_test)\n",
    "          sp = np.sqrt(np.sqrt(pd*(1-fa)) * (0.5*(pd+(1-fa))))\n",
    "          knee = np.argmax(sp)\n",
    "\n",
    "          d['output_test'].append(y_pred_test.tolist()) # Convert to list\n",
    "          d['mse_test'].append(mean_squared_error(y[test], y_pred_test))\n",
    "          d['sp_test'].append(sp[knee])\n",
    "          d['pd_test'].append(pd[knee])\n",
    "          d['pf_test'].append(fa[knee])\n",
    "          d['thr_test'].append(thr[knee])\n",
    "          d['roc_sp_test'].append(sp.tolist()) #\n",
    "          d['roc_pd_test'].append(pd.tolist()) #\n",
    "          d['roc_pf_test'].append(fa.tolist()) #\n",
    "          d['roc_thr_test'].append(thr.tolist()) #\n",
    "\n",
    "          size_sig_test = np.max(np.argwhere(y[test]==1))+1\n",
    "          size_bkg_test = np.size(y[test]) - size_sig_test\n",
    "\n",
    "          d['size_sig_test'].append(size_sig_test)\n",
    "          d['size_bkg_test'].append(size_bkg_test)\n",
    "\n",
    "          d['init'].append(init)\n",
    "          init = init+1\n",
    "\n",
    "          if init > numInit-1:\n",
    "            init = 0\n",
    "\n",
    "\n",
    "    for key in d:\n",
    "        if isinstance(d[key], list) and key != 'model':  # Exclude the 'model' key as it contains JSON strings\n",
    "            try:\n",
    "                d[key] = np.array(d[key], dtype=object)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert '{key}' to NumPy array due to inconsistent shapes.\")\n",
    "\n",
    "\n",
    "    return d\n",
    "\n",
    "def dumpModelFix(models, history, skf, x, y, seed_cv, numInit):\n",
    "    vars = ['model','fold','init','history','seed_cv','output_test','output_train',\n",
    "            'sp_test','sp_train','pd_test','pd_train','pf_test','pf_train',\n",
    "            'thr_train','thr_test','roc_sp_test','roc_sp_train','roc_pd_test',\n",
    "            'roc_pf_test','roc_pd_train','roc_pf_train','roc_thr_test','roc_thr_train',\n",
    "            'mse_train','mse_test','size_sig_test','size_bkg_test']\n",
    "\n",
    "    d = { key:[] for key in vars }\n",
    "    init = 0\n",
    "    h_idx = 0  # índice separado pra history\n",
    "\n",
    "    for i, (train, test) in enumerate(skf.split(x, y)):\n",
    "        for model_idx, model in enumerate(models):\n",
    "            print(f\"Dumping Fold {i}:\")\n",
    "            print(f\"Dumping Init {init}:\")\n",
    "\n",
    "            y_pred_test = model.predict(x[test])\n",
    "            y_pred_train = model.predict(x[train])\n",
    "\n",
    "            d['model'].append(model.to_json())\n",
    "            d['fold'].append(i)\n",
    "            d['init'].append(init)\n",
    "            d['seed_cv'].append(seed_cv)\n",
    "            d['history'].append(history[h_idx].history)\n",
    "            h_idx += 1\n",
    "\n",
    "            # Métricas treino\n",
    "            fa, pd, thr = roc_curve(y[train], y_pred_train)\n",
    "            sp = np.sqrt(np.sqrt(pd * (1 - fa)) * (0.5 * (pd + (1 - fa))))\n",
    "            knee = np.argmax(sp)\n",
    "            d['output_train'].append(y_pred_train.tolist())\n",
    "            d['mse_train'].append(mean_squared_error(y[train], y_pred_train))\n",
    "            d['sp_train'].append(sp[knee])\n",
    "            d['pd_train'].append(pd[knee])\n",
    "            d['pf_train'].append(fa[knee])\n",
    "            d['thr_train'].append(thr[knee])\n",
    "            d['roc_sp_train'].append(sp.tolist())\n",
    "            d['roc_pd_train'].append(pd.tolist())\n",
    "            d['roc_pf_train'].append(fa.tolist())\n",
    "            d['roc_thr_train'].append(thr.tolist())\n",
    "\n",
    "            # Métricas teste\n",
    "            fa, pd, thr = roc_curve(y[test], y_pred_test)\n",
    "            sp = np.sqrt(np.sqrt(pd * (1 - fa)) * (0.5 * (pd + (1 - fa))))\n",
    "            knee = np.argmax(sp)\n",
    "            d['output_test'].append(y_pred_test.tolist())\n",
    "            d['mse_test'].append(mean_squared_error(y[test], y_pred_test))\n",
    "            d['sp_test'].append(sp[knee])\n",
    "            d['pd_test'].append(pd[knee])\n",
    "            d['pf_test'].append(fa[knee])\n",
    "            d['thr_test'].append(thr[knee])\n",
    "            d['roc_sp_test'].append(sp.tolist())\n",
    "            d['roc_pd_test'].append(pd.tolist())\n",
    "            d['roc_pf_test'].append(fa.tolist())\n",
    "            d['roc_thr_test'].append(thr.tolist())\n",
    "\n",
    "            # Tamanhos\n",
    "            size_sig_test = np.sum(y[test] == 1)\n",
    "            size_bkg_test = np.sum(y[test] == 0)\n",
    "            d['size_sig_test'].append(size_sig_test)\n",
    "            d['size_bkg_test'].append(size_bkg_test)\n",
    "\n",
    "            init = (init + 1) % numInit\n",
    "\n",
    "    # Conversão final\n",
    "    for key in d:\n",
    "        if isinstance(d[key], list) and key != 'model':\n",
    "            try:\n",
    "                d[key] = np.array(d[key], dtype=object)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert '{key}' to NumPy array due to inconsistent shapes.\")\n",
    "\n",
    "    return d\n",
    "\n",
    "def save_cv_models(models, histories, filename=\"cv_models.pkl\"):\n",
    "    \"\"\"\n",
    "    Save cross-validated models and training histories to a single file.\n",
    "    \n",
    "    Args:\n",
    "        models (list): List of trained models (e.g., model_mlp_cv).\n",
    "        histories (list): List of History objects (e.g., history_model).\n",
    "        filename (str): Output filename.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"models\": models,\n",
    "        \"histories\": [h.history for h in histories]  # Only the history dict, not the callback object\n",
    "    }\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeT6DHBhxSVP"
   },
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:33.928937Z",
     "start_time": "2025-04-12T16:19:30.614377Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZycQsn1oK6t",
    "outputId": "b84f3cb3-3362-4914-ebfe-005d5fb75d8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['avgmu', 'L2Calo_ring_0', 'L2Calo_ring_1', 'L2Calo_ring_2',\n",
       "       'L2Calo_ring_3', 'L2Calo_ring_4', 'L2Calo_ring_5', 'L2Calo_ring_6',\n",
       "       'L2Calo_ring_7', 'L2Calo_ring_8', 'L2Calo_ring_9',\n",
       "       'L2Calo_ring_10', 'L2Calo_ring_11', 'L2Calo_ring_12',\n",
       "       'L2Calo_ring_13', 'L2Calo_ring_14', 'L2Calo_ring_15',\n",
       "       'L2Calo_ring_16', 'L2Calo_ring_17', 'L2Calo_ring_18',\n",
       "       'L2Calo_ring_19', 'L2Calo_ring_20', 'L2Calo_ring_21',\n",
       "       'L2Calo_ring_22', 'L2Calo_ring_23', 'L2Calo_ring_24',\n",
       "       'L2Calo_ring_25', 'L2Calo_ring_26', 'L2Calo_ring_27',\n",
       "       'L2Calo_ring_28', 'L2Calo_ring_29', 'L2Calo_ring_30',\n",
       "       'L2Calo_ring_31', 'L2Calo_ring_32', 'L2Calo_ring_33',\n",
       "       'L2Calo_ring_34', 'L2Calo_ring_35', 'L2Calo_ring_36',\n",
       "       'L2Calo_ring_37', 'L2Calo_ring_38', 'L2Calo_ring_39',\n",
       "       'L2Calo_ring_40', 'L2Calo_ring_41', 'L2Calo_ring_42',\n",
       "       'L2Calo_ring_43', 'L2Calo_ring_44', 'L2Calo_ring_45',\n",
       "       'L2Calo_ring_46', 'L2Calo_ring_47', 'L2Calo_ring_48',\n",
       "       'L2Calo_ring_49', 'L2Calo_ring_50', 'L2Calo_ring_51',\n",
       "       'L2Calo_ring_52', 'L2Calo_ring_53', 'L2Calo_ring_54',\n",
       "       'L2Calo_ring_55', 'L2Calo_ring_56', 'L2Calo_ring_57',\n",
       "       'L2Calo_ring_58', 'L2Calo_ring_59', 'L2Calo_ring_60',\n",
       "       'L2Calo_ring_61', 'L2Calo_ring_62', 'L2Calo_ring_63',\n",
       "       'L2Calo_ring_64', 'L2Calo_ring_65', 'L2Calo_ring_66',\n",
       "       'L2Calo_ring_67', 'L2Calo_ring_68', 'L2Calo_ring_69',\n",
       "       'L2Calo_ring_70', 'L2Calo_ring_71', 'L2Calo_ring_72',\n",
       "       'L2Calo_ring_73', 'L2Calo_ring_74', 'L2Calo_ring_75',\n",
       "       'L2Calo_ring_76', 'L2Calo_ring_77', 'L2Calo_ring_78',\n",
       "       'L2Calo_ring_79', 'L2Calo_ring_80', 'L2Calo_ring_81',\n",
       "       'L2Calo_ring_82', 'L2Calo_ring_83', 'L2Calo_ring_84',\n",
       "       'L2Calo_ring_85', 'L2Calo_ring_86', 'L2Calo_ring_87',\n",
       "       'L2Calo_ring_88', 'L2Calo_ring_89', 'L2Calo_ring_90',\n",
       "       'L2Calo_ring_91', 'L2Calo_ring_92', 'L2Calo_ring_93',\n",
       "       'L2Calo_ring_94', 'L2Calo_ring_95', 'L2Calo_ring_96',\n",
       "       'L2Calo_ring_97', 'L2Calo_ring_98', 'L2Calo_ring_99', 'L2Calo_et',\n",
       "       'L2Calo_eta', 'L2Calo_phi', 'L2Calo_reta', 'L2Calo_eratio',\n",
       "       'L2Calo_f1', 'et', 'eta', 'phi', 'eratio', 'reta', 'rphi', 'f1',\n",
       "       'f3', 'rhad', 'rhad1', 'wtots1', 'weta1', 'weta2', 'e277',\n",
       "       'deltaE', 'el_lhtight', 'el_lhmedium', 'el_lhloose', 'el_lhvloose',\n",
       "       'deltaR', 'eeMass', 'T0HLTElectronT2CaloTight',\n",
       "       'T0HLTElectronT2CaloMedium', 'T0HLTElectronT2CaloLoose',\n",
       "       'T0HLTElectronT2CaloVLoose', 'T0HLTElectronRingerTight_v8',\n",
       "       'T0HLTElectronRingerMedium_v8', 'T0HLTElectronRingerLoose_v8',\n",
       "       'T0HLTElectronRingerVeryLoose_v8'], dtype='<U31')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_path = 'dataset/data17_13TeV.AllPeriods.sgn.probes_lhmedium_EGAM1.bkg.VProbes_EGAM7.GRL_v97_et%i_eta%i.npz'\n",
    "iet = 1\n",
    "ieta = 0\n",
    "\n",
    "samples = np.load(data_path %(iet, ieta))\n",
    "\n",
    "data = samples['data']\n",
    "features = samples['features']\n",
    "target = samples['target']\n",
    "ksamples_Sig = np.max(np.argwhere(target==1)) +1\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7j60J0Sxb_V"
   },
   "source": [
    "#### Normalizing Inputs (Norm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:34.061028Z",
     "start_time": "2025-04-12T16:19:33.942327Z"
    },
    "id": "4ofoHWPnoAqP"
   },
   "outputs": [],
   "source": [
    "# dataset = np.concatenate((norm1(data[0:ksamples_Sig,1:101]),norm1(data[ksamples_Sig:,1:101])),axis=0)\n",
    "# reduce sample to test\n",
    "dataset = np.concatenate(((data[0:ksamples_Sig,1:101]),data[ksamples_Sig:,1:101]),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7slAlcQSxh7T"
   },
   "source": [
    "#### Building neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPKiplECoAtB",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "models = get_model_conv_rp(100,kernel_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kekhnwVaxsJg"
   },
   "source": [
    "#### Preparing stratified k-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:14.839756709Z",
     "start_time": "2025-04-12T15:41:24.448440Z"
    },
    "id": "Tl7LUp1XoAvm"
   },
   "outputs": [],
   "source": [
    "random_state_cv = 7\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = random_state_cv, shuffle = True)\n",
    "numInit = 10\n",
    "history_model = []\n",
    "model_mlp_cv = []\n",
    "\n",
    "batch = 1024\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhpjJWISxy81"
   },
   "source": [
    "#### Tunning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:14.841572173Z",
     "start_time": "2025-04-12T15:41:24.475400Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_folds = skf.get_n_splits(dataset, target)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(dataset, target)):\n",
    "    for model in models:\n",
    "        callback = sp() #customized callback\n",
    "        callback.set_validation_data((dataset[test_index], target[test_index])) # Set validation data here\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "            dataset[train_index],\n",
    "            target[train_index],\n",
    "            batch_size = batch,\n",
    "            epochs=num_epochs,\n",
    "            verbose=2,\n",
    "            validation_data=(dataset[test_index], target[test_index]),callbacks=[callback]\n",
    "        )\n",
    "        history_model.append(history)\n",
    "        model_mlp_cv.append(model)\n",
    "    percent = (i + 1) / total_folds * 100\n",
    "    print(f\"Progress: {i + 1} / {total_folds} ({percent:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HyyFelZx8ut"
   },
   "source": [
    "#### Extracting results and properties of the tuned models - saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:14.842248706Z",
     "start_time": "2025-04-12T16:12:54.000220Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "XQqvV8NNoA09",
    "outputId": "046201df-1f9a-4a40-f138-ca798bbe06df"
   },
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     model_summary = dumpModel(models,history_model,skf,dataset,target,seed_cv=random_state_cv,numInit=numInit) # Added numInit=numInit to fix missing argument\n",
    "# finally:\n",
    "#     log_file.close()\n",
    "\n",
    "#filename = f\"model_summary_et{iet}_eta{ieta}.pkl\"\n",
    "model_summary = dumpModelFix(models,history_model,skf,dataset,target,seed_cv=random_state_cv,numInit=numInit)\n",
    "#save_cv_models(model_mlp_cv, history_model, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:14.842560342Z",
     "start_time": "2025-04-11T15:10:17.274Z"
    },
    "id": "ap7XWWvJotv6"
   },
   "outputs": [],
   "source": [
    "filename = 'model_summary_cnn_rpringer_et%i_eta%i.npz' % (iet, ieta)\n",
    "np.savez(filename, **model_summary)\n",
    "    \n",
    "\n",
    "#saving in your drive -- reminder to modify the path\n",
    "!cp {filename} models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:19:14.842824348Z",
     "start_time": "2025-04-11T15:10:17.275Z"
    },
    "id": "zosC4_XEnvXX"
   },
   "outputs": [],
   "source": [
    "model_summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
